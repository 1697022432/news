<!--1621246262000-->
[FT社评：慎用情感人工智能](https://cn.ft.com/story/001092523?full=y)
------

<div></div><div class="story-lead">情感人工智能确实具有潜在前景，但我们有理由加倍谨慎。</div><div class=" story-image image"><img src="https://thumbor.ftacademy.cn/unsafe/1340x754/https://thumbor.ftacademy.cn/unsafe/picture/8/000087868_piclink.jpg"></div><div class="story-body"><div id="story-body-container"><p>想象一下，在一场面试中，你面对的不仅是一组面试官，还有一双与电脑相连的电子“眼睛”。这台电脑会使用人工智能，根据你面部和眼睛微小的动作以及你回答问题时语调的变化来评估你的情绪反应，并得出关于你的结论。你可靠吗？你真的想得到这份工作吗？当被问及你最大的失败时，你微微咬紧牙关，这是否表明你有什么要隐瞒的？</p><p>这根本不是一个令人不安的未来设想，而是已经在发生的事情。许多公司正在开发或营销“情绪识别”招聘技术，一些公司已经开始使用这种技术。正如英国《金融时报》本周突出报道的，“情感人工智能”正被用于从广告、游戏到保险，以及执法和安保等多个行业。它展示了这样一种前景：利用面部线索来确定要向人们推销什么以及他们会如何回应广告；查看司机、学生或在家办公的人是否在集中注意力；或者是发现形迹可疑的人。</p><p>历史上充斥着有关新技术的悲观预测，但事实证明这些预测过于危言耸听。然而，对于面部识别技术——情感人工智能与之密切相关——我们有理由加倍谨慎。通过人脸的物理特征将人脸图像与数据库进行匹配的技术大体上是可靠的。然而，即便如此，系统有时也会错误识别女性或非白人的面孔，这可能会导致歧视行为，例如当执法部门使用这种技术时。</p><p>批评人士指责称，情绪识别的基本假设——人类有一套通用情绪，并以相似的方式表现出来——是有很大缺陷的，忽视了文化间、甚至个人之间的差异。2019年的研究指出，比如说，扬起的眉毛背后有各种各样的含义。如果在某些文化中特别普遍的面部反应被认为是说谎或紧张的迹象，那么这最终可能会加强不准确的刻板印象。</p><div  data-o-ads-name="mpu-middle1" class="o-ads in-article-advert" data-o-ads-formats-default="false"  data-o-ads-formats-small="FtcMobileMpu"  data-o-ads-formats-medium="FtcMpu" data-o-ads-formats-large="FtcMpu" data-o-ads-formats-extra="FtcMpu" data-o-ads-targeting="cnpos=middle1;" data-cy='[{"devices":["PC","iPhoneWeb","AndroidWeb","iPhoneApp","AndroidApp"],"pattern":"MPU","position":"Middle1","container":"mpuInStory"}]'></div><p>试验表明，人工智能系统在辨别人类是否在说真话方面远非完美。然而，该技术有潜在的有益用途——例如，帮助自闭症患者从他人的面部线索中解读情绪。</p><p>欧盟委员会(European Commission)提议部分地禁止执法部门在公共场所使用远程生物识别监控技术，如面部识别——尽管人权组织已经对这些技术存在的漏洞发出警告，欧盟自己的隐私监管机构也表示，应该全面禁止在公共场所使用这些技术。这样做可能太过分了。但暂停使用情感人工智能是很有道理的。这样一来，人们就有时间来确定它背后的科学原理是否可靠，它最适合在什么样的环境和应用中使用，以及如何对其进行最佳监管。</p><p>即使情感识别最终被允许在一些公共场所使用，但在它运行时也应当告知公众。如果在涉及个人的情况下使用，则应征得他们的同意，而且他们应该有权选择退出。当涉及到招聘或绩效等决策时，情感人工智能永远不应取代人类的判断，只能为其提供辅助。</p><p>如果情感识别能够以可靠且可信的方式工作，在最好的情况下，它能提供一种技术人性化的方式，并帮助企业更深入地了解客户。最坏的情况是，它可能成为一种监控资本主义的侵入性工具。人们必须找到其中的平衡点。</p><p>译者/何黎</p></div><div class="clearfloat"></div></div>
