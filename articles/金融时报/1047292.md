<!--1600980751000-->
[艺术展：AI的歧视、缺陷与风险](https://cn.ft.com/story/001089557?full=y)
------

<div></div><div class="story-lead">宋佩芬：麦克阿瑟“天才奖”得主Trevor Paglen 9月在伦敦佩斯画廊的个展，从AI训练过程出发，探讨AI如何延续种族主义、阶级分化等偏见。</div><div class=" story-image image"><img src="https://thumbor.ftacademy.cn/unsafe/1340x754/https://thumbor.ftacademy.cn/unsafe/picture/4/000101374_piclink.jpg"></div><div class="story-body"><div id="story-body-container"><p>威廉姆斯(Robert Williams)住在底特律郊外，今年6月某天在家被警方逮捕，理由是脸部识别软件认为他涉嫌一起偷窃案——一名黑人闯入珠宝店，偷走了一些手表。闭路电视摄像头只拍到一点点嫌犯，而且非常不清楚。虽然警方知道，他们所使用的脸部识别软件有高达96%的错误率，而且软件也显示，对威廉姆斯只有六七成的把握，警方依旧去逮捕他，而且将之拘留了30个小时，直到他被证明无辜。这个事件曝露了AI的偏差，认为大多数的嫌犯可能是宽鼻梁、低眉毛，从偏见出发，导致脸部识别软件错误百出。</p><p>AI怎么可能有偏见？麦克阿瑟“天才奖”得主Trevor Paglen 今年9月在伦敦佩斯画廊(Pace)的个展直接面对这个问题。从AI的训练过程出发，探讨“受训过”的AI如何反映并延续了种族主义、父权制和阶级分化等社会不平等。不论是治安单位，还是大型企业，利用AI做脸部识别以及监控可能导致什么危险。</p><p>被Artnet 称为“超级英雄式的实证主义艺术家”，Paglen的专长是透过看得见，来调查看不见的。他不但为了亲眼看到埋在海底的网路电缆而去学潜水，还由于对美国公安局的“深入观察”，参与斯诺登纪录片《第四公民》（Citizenfour）的拍摄。  他在佩斯画廊展览的主题是《盛开》。“这是我在今年春天，当世界充满危机，充满了恐惧悲伤时创作的。疫情不但揭露生命的脆弱，也揭露出制度的脆弱。”Paglen在越洋电话中解释，他所住的纽约虽然像个空城，但春天的花朵压倒性地绽放，呈现出大自然的爆发力。“在艺术史上，花朵代表了多层的意义，既可能象征脆弱，生命的短暂，也能够代表生命的降临。”然而，不像在公园所看到的，展览中花朵看起来相当不自然，尤其是它的颜色。Paglen 解释，花朵的颜色是AI分析了不同的图像之后所生成的，是AI眼中而不是现实生活中的颜色。<img src="https://thumbor.ftacademy.cn/unsafe/picture/5/000101375_piclink.jpg" /></p><p>训练AI辨识图像，你首先必须为它输入上千万图片。斯坦福大学开发的图片网ImageNet就是采用了文字数据库WordNet上的字条，为每一个字条找到对应的图像来输入AI。帮AI喂食这么多图片之后，接下来的工作是进行分类，像汽车、植物、水果等，来帮AI消化学习。由于WordNet的结构，输入“APPLE”就可能出现苹果、苹果蚜虫、苹果酱、苹果馅饼、苹果天竺葵、苹果果冻、苹果汁、苹果蛆、苹果锈、苹果树、苹果派、苹果酒等结果。“HOT”则会出现热线、热裤、热盘、热锅、热棒、辣酱、温泉、热奶茶、热浴盆、热气球和热水瓶等图片。ImageNet上有大约1500万张图片，被划分成22000个类别。当然了，从植物、水果、电器用品来说，这些分类一目了然。但是在这22000个类别中有2833个是针对人的分类，有直截了当的，如潜水员、啦啦队长等，但也有充满种族主义，和厌恨女性等残忍的类别，如荡妇、坏人、败家子等等。</p><div  data-o-ads-name="mpu-middle1" class="o-ads in-article-advert" data-o-ads-formats-default="false"  data-o-ads-formats-small="FtcMobileMpu"  data-o-ads-formats-medium="FtcMpu" data-o-ads-formats-large="FtcMpu" data-o-ads-formats-extra="FtcMpu" data-o-ads-targeting="cnpos=middle1;" data-cy='[{"devices":["PC","iPhoneWeb","AndroidWeb","iPhoneApp","AndroidApp"],"pattern":"MPU","position":"Middle1","container":"mpuInStory"}]'></div><p>去年，Paglen和纽约大学教授Kate Crawford利用ImageNet的分类结构，开发了《图像轮盘赌》 （ImageNet Roulette）的APP，你只要上传自己的照片，AI就会用ImageNet分类方式来判断你是什么样的人。《图像轮盘赌》一出现，立刻有成千上万的人要AI帮他们看相，有些人被断定是“成年男子”，或“保姆”，但是有些人获得的评论就相当恶毒，英国《卫报》一位亚裔记者将员工证上传，AI描述她是“斜眼的外国人”，具有“对亚洲人蔑称（特别是对越战中的北越士兵）”的人格！</p><p>“AI会做出这些判断的原因完全来自分类本身缺乏公平性。”Paglen 解释，“这些仅仅反映特殊族群的利益和他们的世界观，但程式设计者没有去解释这一点，让人以为世界就是这样，如此就产生了偏见的问题。”Paglen 继续分析ImageNet如何将人们在社交媒体上传的图片进行分类，“这些生活片刻，我们和亲朋好友分享的东西，也在和科技公司分享。科技公司是想根据我们的照片和发表的文字，对我们进行分类，利用这些分类，来从我们身上榨取钱财，或者对我们进行监管。”</p><p>展览中两件作品《分心驾驶》（Distracted Drivers）和《步调的分类》（Classifications of Gait）都是由数千张图像所组成的数据网格。《分心驾驶》探讨AI如何识别驾驶人是否分心，用来侦测并调整他们的保险费。这个数据集是由State Farm保险公司开发的，如果你在没有发生任何事故的情况下，保险费突然高涨，很可能就是充满偏见的AI加上大规模监控的结果。不仅仅是在路上开车，你的手机可以记录你的睡眠时间，如果你没睡饱，医疗保单很可能因此上涨。</p><p>我问Paglen,科技能够帮忙维持治安，追踪传染病，是否可能在受益于科技和享受隐私之间找到一个平衡点？“这是很复杂的问题，我们必须先质询图像识别潜在的偏见。”他说，“我认为做出一个没有偏见的数据集是不可能的，分类方法永远是少数人的世界观，不可能完全公正公平。而且，什么是公平？什么是公正？这些问题在不同的情况下会有不同的答案。这就是这类科技最根本问题。”他又提出，谁有权力看谁？谁有权力根据电脑图像来判断一个人？谁有权力根据一个人的脸部特征来决定他是否有罪？随着工作越来越数字化，越来越多衡量生产力、注意力、工作效果的监控工具将被植入我们的使用平台，这些信息将会被传输到保险公司，到脸书谷歌亚马逊，无形中，你的命运已经被AI主宰。<img src="https://thumbor.ftacademy.cn/unsafe/picture/6/000101376_piclink.jpg" /></p><p>早在数十年前，好莱坞就已经透过《银翼杀手》、《魔鬼终结者》来警告人们机器崛起的危险，在今天，我们更加需要像Paglen这类的艺术家来提醒世界，无所不在的AI 不但充满缺陷，还有潜在的威胁。人类应该觉醒，对AI 进行严格监管，刻不容缓！</p><p>（本文仅代表作者本人观点。未经许可，不得转载。责编邮箱：shirley.xue@ftchinese.com）</p></div><div class="clearfloat"></div></div>
